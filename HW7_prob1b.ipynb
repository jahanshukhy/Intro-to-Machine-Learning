{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuR/eqXPqs1DVjIPPyp2hB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jahanshukhy/Intro-to-Machine-Learning/blob/main/HW7_prob1b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports & device\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "apfdkHKF17pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "wKX3DCUp7gzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download CIFAR-10, convert to tensor and normalize\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2470, 0.2435, 0.2616]\n",
        "    ),\n",
        "])\n",
        "\n",
        "data_path = './data-cifar10/'\n",
        "\n",
        "cifar10_train = datasets.CIFAR10(\n",
        "    root=data_path, train=True, download=True, transform=transform\n",
        ")\n",
        "\n",
        "cifar10_val = datasets.CIFAR10(\n",
        "    root=data_path, train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "trainloader = DataLoader(cifar10_train, batch_size=128, shuffle=True)\n",
        "valloader   = DataLoader(cifar10_val,   batch_size=128, shuffle=False)\n",
        "\n",
        "print(\"Train size:\", len(cifar10_train))\n",
        "print(\"Val size  :\", len(cifar10_val))\n"
      ],
      "metadata": {
        "id": "3aJEHj7g19EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Model\n",
        "\n",
        "class CNN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolution layer 1\n",
        "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Convolution layer 2\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
        "\n",
        "        # Convolution layer 3\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "\n",
        "        # After 3 poolings: 32 -> 16 -> 8 -> 4\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))   # 3→8, pool 32→16\n",
        "        x = self.pool(F.relu(self.conv2(x)))   # 8→16, pool 16→8\n",
        "        x = self.pool(F.relu(self.conv3(x)))   # 16→32, pool 8→4\n",
        "        x = x.view(-1, 32 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "re23dVlM2BVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Model, Loss, Optimizer\n",
        "\n",
        "model = CNN_Model().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def count_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model Size (Trainable Parameters): {total_params}\")\n",
        "    return total_params"
      ],
      "metadata": {
        "id": "XF4dUNSy2E0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function\n",
        "\n",
        "def train_model(num_epochs=10):\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Training loop\n",
        "        for imgs, labels in trainloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss_epoch = running_loss / len(trainloader)\n",
        "        train_acc_epoch = correct / total\n",
        "\n",
        "        train_losses.append(train_loss_epoch)\n",
        "        train_accuracies.append(train_acc_epoch)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        v_loss = 0.0\n",
        "        v_correct = 0\n",
        "        v_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in valloader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                v_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                v_total += labels.size(0)\n",
        "                v_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss_epoch = v_loss / len(valloader)\n",
        "        val_acc_epoch  = v_correct / v_total\n",
        "\n",
        "        val_losses.append(val_loss_epoch)\n",
        "        val_accuracies.append(val_acc_epoch)\n",
        "\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        print(f\"Epoch {epoch+1:4d} | \"\n",
        "              f\"Time: {epoch_time:6.2f} s | \"\n",
        "              f\"Train loss: {train_loss_epoch:.4f} | \"\n",
        "              f\"Val loss: {val_loss_epoch:.4f} | \"\n",
        "              f\"Train acc: {train_acc_epoch:.4f} | \"\n",
        "              f\"Val acc: {val_acc_epoch:.4f}\")\n",
        "\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTraining finished in {total_time/60:.2f} minutes.\")\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n"
      ],
      "metadata": {
        "id": "OiXBseJm2KDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Function\n",
        "\n",
        "def plot_curves(train_losses, val_losses, train_acc, val_acc):\n",
        "\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 12))\n",
        "\n",
        "    # training vs validation loss plot\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(epochs, train_losses, label=\"Train loss\")\n",
        "    plt.plot(epochs, val_losses, label=\"Validation loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # training vs validation accuracy plot\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(epochs, train_acc, label=\"Train accuracy\")\n",
        "    plt.plot(epochs, val_acc, label=\"Validation accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Training vs Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "HAneJU6D15vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"\\n--- MODEL SIZE ---\")\n",
        "    count_parameters(model)\n",
        "\n",
        "    print(\"\\n--- TRAINING ---\")\n",
        "    train_losses, val_losses, train_acc, val_acc = train_model(num_epochs=300)\n",
        "\n",
        "    print(\"\\n--- PLOTTING ---\")\n",
        "    plot_curves(train_losses, val_losses, train_acc, val_acc)\n"
      ],
      "metadata": {
        "id": "9-JMEKid2M6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}